{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc0dbb2",
   "metadata": {},
   "source": [
    "# Quantum Support Vector Machine (QSVM)\n",
    "\n",
    "QSVM is a Quantum Machine Learning (QML) algorithm designed for binary classification, inspired by the classical Support Vector Machine (SVM). In the following, we will briefly review the SVM algorithm and then study its quantum version. After that, you will be ready to apply the QSVM to two example datasets from Scikit-learn.\n",
    "\n",
    "### 1. Overview of SVM algorithm\n",
    "\n",
    "Let's suppose that we are given a set of points $x \\in \\mathbb{R}^2$ lying on a plane, each of them with a label $y \\in \\{0, 1\\}$. The goal is to find a line that separates the points of different labels (see Figure below, taken from [3], where the color indicates the binary label). As you may have noticed, if such a line exists, which sometimes does not, there are infinite lines that do the job. However, some are better than others, as they have a greater margin from the closest points from each class.\n",
    "\n",
    "![Separating line](figures_qsvm/svm_separating_lines.png)\n",
    "![Margin lines](figures_qsvm/svm_margin_hyperplanes.png)\n",
    "\n",
    "\n",
    "So far we have introduced a linear classifier, but the SVM is more than that, it is a ML algorithm (see [1] for an informal introduction to ML or [2] if you are interested in delving more into it). Thus, it does not only aim to classify the given points but tries to learn the underlying probability distribution P(y|x) and, after training is completed, be able to classify correctly unseen points (hoping that they follow the same probability distribution). Therefore, the separating line is more likely to generalize to unseen data if it has a wider margin to the closest points in the training data.\n",
    "\n",
    "As you may recall from High School, the equation of a line can be expressed as: $\\boldsymbol{w} \\cdot \\boldsymbol{x}+b=0$, where $w$ is a vector normal to the line. We can also define the equation of a parallel line that passes through the closest point to the separating line, as well as the equation associated to the symmetric line. Normalizing these equations, they can be expressed as: $\\boldsymbol{w} \\cdot \\boldsymbol{x}+b = \\pm 1$, where the distance between them can be shown to be $2/||\\boldsymbol{w}||$. Hence, the goal is to maximize this margin, i.e., solve the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Minimize} \\quad & \\frac{1}{2} \\| w\\|^2, \\\\\n",
    "\\text{subject to} \\quad & y_j \\left( \\vec{w} \\cdot \\vec{x}_j + b \\right) \\geq 1, \\quad \\forall j.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This optimization problem is known as the hard margin case, as no point is allowed to be misclassified or lie inside the margin. However, ML datasets usually contain noise and the goal of ML algorithms is to distinguish noise from important information. Thus, the previous constraint is relaxed, and the optimization problem that SVM actually solves is the following (we skip the derivation here, but it can be found in [4]):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Maximize} \\quad & \\sum_{j} \\alpha_j - \\frac{1}{2} \\sum_{j,k} y_j y_k \\alpha_j \\alpha_k \\left( \\vec{x}_j \\cdot \\vec{x}_k \\right), \\\\\n",
    "\\text{subject to} \\quad & 0 \\leq \\alpha_j \\leq C, \\quad \\forall j, \\\\\n",
    "& \\sum_{j} \\alpha_j y_j = 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is a convex quadratic programming problem, which can be solved using Sequential Minimal Optimization (SMO) [5]. Finally, we note that the SVM does not only apply to datasets with two features (or points in a plane), this was just done for visualization purposes, but can be applied to datasets with arbitrary number of features. Thus, in the general case, the separating line becomes an hyperplane, but the mathematics do not change.\n",
    "\n",
    "### 2. Feature map\n",
    "\n",
    "The algorithm defined so far can only classify data satisfactorily if it is linearly separable. This is rarely the case, especially for datasets with many features. There is a trick though, which consists on mapping the data to a higher dimensional space (known as feature map), and hope that in this space the data points are linearly separable (see Figure below). This procedure is known as the Kernel trick in the literature, as $\\vec{x}_j \\cdot \\vec{x}_k $ is replaced by $K(\\vec{x}_j, \\vec{x}_k) = \\langle \\phi(\\vec{x}_j)|\\phi(\\vec{x}_k) \\rangle$, where $\\phi$ denotes the feature map. Applying the kernel for all the possible pairs of datapoints defines the kernel matrix.\n",
    "\n",
    "![feature map](figures_qsvm/feature%20map.png)\n",
    "\n",
    "\n",
    "### 3. Quantum SVM\n",
    "\n",
    "Having explained the classical SVM algorithm, we are ready to go Quantum. The idea is to use a parametrized quantum circuit as the feature map, and make use of the high dimensionality of the Hilbert space ($2^{n}$ where $n$ is the number of qubits). There are many different ways to map vectors to quantum states [3] but here we will explain and implement the angle encoding feature map. It consists of, starting with $n$ qubits (which must coincide with the number of features of the dataset) in state $|0 \\rangle$, applying a rotation gate to each qubit, where the angle for each gate is the value of the corresponding feature.\n",
    "\n",
    "Since what we need is the kernel matrix (composed by the inner product of vectors in feature space), we need to calculate the following: $K(\\vec{x}_j, \\vec{x}_k) = \\langle \\phi(\\vec{x}_j),\\phi(\\vec{x}_k) \\rangle = |\\langle 0|\\phi ^\\dagger(\\vec{x}_j)\\phi(\\vec{x}_k)|0\\rangle |^2$. Hence, after applying the feature map to the first vector, we need to apply the inverse feature map (replacing every gate by its inverse) to the second vector. Finally, we will execute the quantum circuit $n_{shots}$ times (measuring in the computational basis) and estimate the probability of measuring all qubits in the $|0\\rangle$ state.\n",
    "\n",
    "Having calculated the quantum kernel matrix, which requires $n_{shots}\\times n_{samples}^2$ runs of the quantum circuit (with number of qubits = number of features), we can apply the \"normal\" SVC and SMO to find the optimal classifying hyperplane. After the optimal normal vector and bias of the hyperplane is found, i.e. training is complete, the algorithm can predict the label of new data points by calculating: $sign(\\omega^*x_{new} + b^*)$\n",
    "\n",
    "### 4. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e3aea2",
   "metadata": {},
   "source": [
    "This example is inspired by the book \"A Practical Guide to Quantum Machine Learning and Quantum Optimization\" \n",
    "by Elías F.Combarro and Samuel González-Castillo. It uses a dataset for wine classification, which has the following characteristics (see https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html for more details).\n",
    "\n",
    "<img src=\"figures_qsvm/wine_dataset.png\" alt=\"Wine dataset\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abb38b",
   "metadata": {},
   "source": [
    "0. Connect to QAAS if you want to use Qaptiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36a2c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect = False  # Set to False if you want to run locally without QLMaaS\n",
    "\n",
    "if connect:\n",
    "    from qat.qlmaas import QLMaaSConnection\n",
    "    conn = QLMaaSConnection(hostname=\"qlm35e.neasqc.eu\", check_host=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58387442",
   "metadata": {},
   "source": [
    "1. Import the necessary libraries and the self-made QSVM class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78182fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from myqml import QSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ab35f",
   "metadata": {},
   "source": [
    "2. Filter the wine dataset so that there are only two types of wines, yielding a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275521fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of X: (130, 13)\n",
      "Dimension of y: (130,)\n"
     ]
    }
   ],
   "source": [
    "x, y = load_wine(return_X_y=True)\n",
    "x = x[:59+71,:]\n",
    "y = y[:59+71]\n",
    "print(f\"Dimension of X: {x.shape}\")\n",
    "print(f\"Dimension of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166211c8",
   "metadata": {},
   "source": [
    "3. Split the data into training and test sets so that the performance of the model is measured on unseen data, and scale the features using only the training data to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d184178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled training inputs:\n",
      "[[0.81456507 0.25507901 0.77950311 ... 0.76608187 0.68       0.375     ]\n",
      " [0.92717465 0.39051919 0.7484472  ... 0.67251462 0.725      0.78571429]\n",
      " [0.88739042 0.53273138 0.82919255 ... 0.60233918 0.7925     0.70535714]\n",
      " ...\n",
      " [0.79501011 0.48081264 0.86335404 ... 0.56725146 0.61       0.27738095]\n",
      " [0.90964262 0.37471783 0.69565217 ... 0.57309942 0.695      0.28095238]\n",
      " [0.87997303 0.3724605  0.79192547 ... 0.65497076 0.6275     0.6577381 ]]\n",
      "\n",
      "Dimension of training dataset: (104, 13)\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "x_tr, x_test, y_tr, y_test = train_test_split(x, y, train_size = 0.8, random_state=seed)\n",
    "scaler = MaxAbsScaler()\n",
    "x_tr = scaler.fit_transform(x_tr)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_test = np.clip(x_test, 0, 1)\n",
    "print(f\"Scaled training inputs:\\n{x_tr}\")\n",
    "print(f\"\\nDimension of training dataset: {x_tr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b2ce5",
   "metadata": {},
   "source": [
    "4. Since the dataset has 13 features, the quantum circuits require 13 qubits. In order to reduce the number of qubits and thus, the computational complexity, we will perform a dimensionality reduction technique, termed as Principal Component Analysis (PCA). This method first finds the directions of maximum variance (principal components) and then performs a linear transformation, or change of basis, so that the feature space (rows of the dataset) is expressed in the basis of principal components. Finally, the columns associated with the directions of least variance are removed, as they contain the least amount of information. In this tuturial, you can choose the value of n_components, which determines the number of transformed features that are kept. For an introduction to PCA, we suggest reading [6]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f579a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "pca = PCA(n_components = n_components)\n",
    "xs_tr = pca.fit_transform(x_tr)\n",
    "xs_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a04993",
   "metadata": {},
   "source": [
    "5. Define the quantum kernel using angle encoding or ZZ Feature map. For the two datasets studied in this tutorial, angle encoding performs generally better and is faster. Prove it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054f7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the attributes of the QSVM algorithm\n",
    "n_qubits = n_components\n",
    "device = 'myQLM'\n",
    "kernel_circuit_label = \"angle_encoding\" # 'angle encoding' or 'zz_encoding'\n",
    "angle_encoding_type = \"y\" #'x', 'y' or 'z' for 'angle encoding' and any of them or None for 'zz_encoding'\n",
    "\n",
    "# Create a QSVM instance \n",
    "qsvm = QSVM(n_qubits=n_qubits,\n",
    "            device = device,\n",
    "            kernel_circuit_label=kernel_circuit_label,\n",
    "            angle_encoding_type=angle_encoding_type\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee887794",
   "metadata": {},
   "source": [
    "6. Calculate and print the kernel matrix associated with the training data points (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2961b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_kernel_matrix = False # Set to True if you want to print the kernel matrix\n",
    "if print_kernel_matrix:\n",
    "    kernel_matrix = qsvm.qkernel(xs_tr, xs_tr)\n",
    "    print(\"Kernel matrix:\\n\", kernel_matrix)\n",
    "    print(\"\\nDimensions of the Kernel matrix: \", kernel_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046f60c",
   "metadata": {},
   "source": [
    "7. Fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34a68894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QSVM model trained.\n"
     ]
    }
   ],
   "source": [
    "qsvm.fit(xs_tr, y_tr)\n",
    "print(\"QSVM model trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276facee",
   "metadata": {},
   "source": [
    "8. Predict the labels for the test set and calculate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "030f7451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of (transformed) test dataset: (26, 5)\n",
      "Accuracy of QSVM: 0.96\n"
     ]
    }
   ],
   "source": [
    "y_pred = qsvm.predict(xs_test)\n",
    "print(f\"Size of (transformed) test dataset: {xs_test.shape}\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of QSVM: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e150af2b",
   "metadata": {},
   "source": [
    "5. Evaluate the confusion matrix, which shows: the true positives (top left), the false positives (top right), the false negatives (bottom left), and the true negatives (bottom right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b2f3eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[72  0]\n",
      " [ 5 37]]\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Confusion matrix:\\n {conf_mat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e079800",
   "metadata": {},
   "source": [
    "### 5. Use a different dataset: Breast Cancer Detection\n",
    "\n",
    "We now employ a more difficult but still tractable dataset, which determines if a person has breast cancer depending on different characteristics. The details of the dataset are shown in the figure below, taken from the website: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html.\n",
    "\n",
    "<img src=\"figures_qsvm/breast_cancer.png\" alt=\"Breast cancer\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e4207",
   "metadata": {},
   "source": [
    "0. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9133a208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  name     role         type demographic description units  \\\n",
      "0                   ID       ID  Categorical        None        None  None   \n",
      "1            Diagnosis   Target  Categorical        None        None  None   \n",
      "2              radius1  Feature   Continuous        None        None  None   \n",
      "3             texture1  Feature   Continuous        None        None  None   \n",
      "4           perimeter1  Feature   Continuous        None        None  None   \n",
      "5                area1  Feature   Continuous        None        None  None   \n",
      "6          smoothness1  Feature   Continuous        None        None  None   \n",
      "7         compactness1  Feature   Continuous        None        None  None   \n",
      "8           concavity1  Feature   Continuous        None        None  None   \n",
      "9      concave_points1  Feature   Continuous        None        None  None   \n",
      "10           symmetry1  Feature   Continuous        None        None  None   \n",
      "11  fractal_dimension1  Feature   Continuous        None        None  None   \n",
      "12             radius2  Feature   Continuous        None        None  None   \n",
      "13            texture2  Feature   Continuous        None        None  None   \n",
      "14          perimeter2  Feature   Continuous        None        None  None   \n",
      "15               area2  Feature   Continuous        None        None  None   \n",
      "16         smoothness2  Feature   Continuous        None        None  None   \n",
      "17        compactness2  Feature   Continuous        None        None  None   \n",
      "18          concavity2  Feature   Continuous        None        None  None   \n",
      "19     concave_points2  Feature   Continuous        None        None  None   \n",
      "20           symmetry2  Feature   Continuous        None        None  None   \n",
      "21  fractal_dimension2  Feature   Continuous        None        None  None   \n",
      "22             radius3  Feature   Continuous        None        None  None   \n",
      "23            texture3  Feature   Continuous        None        None  None   \n",
      "24          perimeter3  Feature   Continuous        None        None  None   \n",
      "25               area3  Feature   Continuous        None        None  None   \n",
      "26         smoothness3  Feature   Continuous        None        None  None   \n",
      "27        compactness3  Feature   Continuous        None        None  None   \n",
      "28          concavity3  Feature   Continuous        None        None  None   \n",
      "29     concave_points3  Feature   Continuous        None        None  None   \n",
      "30           symmetry3  Feature   Continuous        None        None  None   \n",
      "31  fractal_dimension3  Feature   Continuous        None        None  None   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "5              no  \n",
      "6              no  \n",
      "7              no  \n",
      "8              no  \n",
      "9              no  \n",
      "10             no  \n",
      "11             no  \n",
      "12             no  \n",
      "13             no  \n",
      "14             no  \n",
      "15             no  \n",
      "16             no  \n",
      "17             no  \n",
      "18             no  \n",
      "19             no  \n",
      "20             no  \n",
      "21             no  \n",
      "22             no  \n",
      "23             no  \n",
      "24             no  \n",
      "25             no  \n",
      "26             no  \n",
      "27             no  \n",
      "28             no  \n",
      "29             no  \n",
      "30             no  \n",
      "31             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "  \n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "x = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "\n",
    "# Convert to numpy arrays \n",
    "x = x.to_numpy()\n",
    "y = y.to_numpy()\n",
    "  \n",
    "# variable information \n",
    "print(breast_cancer_wisconsin_diagnostic.variables) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ccbb3",
   "metadata": {},
   "source": [
    "1. Split training and test data, and apply scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94234bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_test, y_tr, y_test = train_test_split(x, y, train_size = 0.8, random_state=seed)\n",
    "scaler = MaxAbsScaler()\n",
    "x_tr = scaler.fit_transform(x_tr)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_test = np.clip(x_test, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd810ca1",
   "metadata": {},
   "source": [
    "2. Apply PCA to reduce the number of features from 30 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2537b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "pca = PCA(n_components = n_components)\n",
    "xs_tr = pca.fit_transform(x_tr)\n",
    "xs_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacaac0d",
   "metadata": {},
   "source": [
    "3. Use angle encoding and fit the training data using QSVM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d55649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a943763\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:746: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y_ = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting completed in  5115.112501382828  s\n"
     ]
    }
   ],
   "source": [
    "n_qubits = n_components\n",
    "device = 'myQLM'\n",
    "kernel_circuit_label = \"angle_encoding\"\n",
    "angle_encoding_type = \"y\"\n",
    "qsvm = QSVM(n_qubits=n_qubits, \n",
    "            device=device, \n",
    "            kernel_circuit_label=kernel_circuit_label,\n",
    "            angle_encoding_type=angle_encoding_type\n",
    "            )\n",
    "start = time.time()\n",
    "qsvm.fit(xs_tr, y_tr)\n",
    "end = time.time()\n",
    "comp_time = end - start\n",
    "print(\"Fitting completed in \", comp_time, ' s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d897a8",
   "metadata": {},
   "source": [
    "4. Predict the labels for the test set and calculate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02ae657c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of (transformed) test dataset: (114, 5)\n",
      "Accuracy of QSVM: 0.96\n"
     ]
    }
   ],
   "source": [
    "y_pred = qsvm.predict(xs_test)\n",
    "print(f\"Size of (transformed) test dataset: {xs_test.shape}\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of QSVM: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcef410",
   "metadata": {},
   "source": [
    "5. Evaluate the confusion matrix, which shows: the true positives (top left), the false positives (top right), the false negatives (bottom left), and the true negatives (bottom right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "020f1842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[72  0]\n",
      " [ 5 37]]\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Confusion matrix:\\n {conf_mat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb3952f",
   "metadata": {},
   "source": [
    "In conclusion, we have shown how quantum computing can be integrated with a basic supervised ML algorithm to learn how to predict labels in datasets. Although we have achieved a very high accuracy for both datasets studied, it is important to note that real-life datasets are way more extensive and complex. Therefore, using this QSVM approach would require the execution of a vast number of quantum circuits of many qubits, which is intractable with current NISQ quantum computers and quantum emulators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ddb30",
   "metadata": {},
   "source": [
    "### 6. Advanced: Use your own custom quantum kernel for QSVM, defining it in myQLM or any other quantum programming language, such as Qiskit, Pennylane or Cirq.\n",
    "\n",
    "Using overriding, you can define a child class of QSVM which defines a different kernel_circuit method using any quantum SDK. As an alternative, only compaible with myQLM, you can define a function that creates the quantum kernel quantum program, and input the function in the custom_kernel argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_QSVM(QSVM):\n",
    "    def kernel_circuit(self, a, b):\n",
    "        pass\n",
    "\n",
    "        '''\n",
    "        TO BE FILLED BY THE USER \n",
    "\n",
    "        Thuis function should return a quantum program that computes the kernel between two vectors a and b, submit the corresonding job to the quantum simulator\n",
    "        or QPU, and return the probability of measuring all qubits in the |0> state.\n",
    "        '''\n",
    "\n",
    "        return probability_all_zeros\n",
    "    \n",
    "def custom_kernel(a, b):\n",
    "\n",
    "    '''\n",
    "    TO BE FILLED BY THE USER (USE myQLM)\n",
    "\n",
    "    This function should return a quantum program that computes the kernel between two vectors a and b.\n",
    "    '''\n",
    "\n",
    "    return qprogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'overriding' # 'overriding' or 'custom_kernel'\n",
    "\n",
    "if method == 'overriding':\n",
    "    custom_qsvm = Custom_QSVM(n_qubits)\n",
    "    custom_qsvm.fit(xs_tr, y_tr)\n",
    "    y_pred = custom_qsvm.predict(xs_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Results using overriding custom kernel...\")\n",
    "    print(f\"Accuracy of QSVM: {accuracy:.2f}\\n\")\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Confusion matrix:\\n {conf_mat}\")\n",
    "\n",
    "else:\n",
    "    custom_qsvm = QSVM(n_qubits=n_qubits, \n",
    "            device=device, \n",
    "            custom_kernel=custom_kernel\n",
    "            )\n",
    "    custom_qsvm.fit(xs_tr, y_tr)\n",
    "    y_pred = custom_qsvm.predict(xs_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Results using custom kernel function argument...\")\n",
    "    print(f\"Accuracy of QSVM: {accuracy:.2f}\\n\")\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Confusion matrix:\\n {conf_mat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b76b39",
   "metadata": {},
   "source": [
    "### 7. References\n",
    "[1] Medium article that explains ML: https://medium.com/@RobuRishabh/introduction-to-machine-learning-555b0f1b62f5\n",
    "\n",
    "[2] Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer\n",
    "\n",
    "[3] Schuld & Killoran, *A Practical Guide to Quantum Machine Learning and Quantum Optimization*\n",
    "\n",
    "[4] Abu-Mostafa et al. *Learning from  Data*, e-Ch. 8\n",
    "\n",
    "[5] Platt, J. (1998). *Sequential minimal optimization: A fast algorithm for training support vector machines*\n",
    "\n",
    "[6] Shlens, J. (2014). *A tutorial on principal component analysis*. arXiv preprint arXiv:1404.1100."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
